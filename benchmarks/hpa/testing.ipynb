{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc94939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import decode_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291a3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.Config.set_tbl_rows(-1)\n",
    "pl.Config.set_tbl_width_chars(-1)\n",
    "pl.Config.set_tbl_cols(-1)\n",
    "pl.Config.set_fmt_table_cell_list_len(-1)\n",
    "pl.Config.set_fmt_str_lengths(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca941a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = os.path.expanduser('/scr/data/cell_crops/')\n",
    "metadata_path = os.path.join(root_dir, 'metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d02b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pl.read_csv(metadata_path)\n",
    "first_entry = metadata[0]\n",
    "first_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d021fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76ae14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_meta = metadata.rows(named=True)\n",
    "print(len(dict_meta))\n",
    "dict_meta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e198c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b5c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "studies = open(\"test_antibodies.txt\", 'r').readlines()\n",
    "studies = [s.strip() for s in studies if s.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d4dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_antibodies = metadata.filter(pl.col('antibody').is_in(studies))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059b87a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plate = str(first_entry['if_plate_id'].item())\n",
    "position = str(first_entry['position'].item())\n",
    "sample = str(first_entry['sample'].item())\n",
    "cell_id = sample = str(int(first_entry['cell_id'].item()))\n",
    "image = os.path.join(root_dir, plate, f\"{plate}_{position}_{sample}_{cell_id}_cell_image.png\")\n",
    "pask = os.path.join(root_dir, plate, f\"{plate}_{position}_{sample}_{cell_id}_cell_mask.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0781117",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3466cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import decode_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "class UnZippedImageArchive(Dataset):\n",
    "    \"\"\"Basic unzipped image arch. This will no longer be used. \n",
    "       Remove when unzipped support is added to the IterableImageArchive\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str= '/scr/data/cell_crops/') -> None:\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata_path = os.path.join(self.root_dir, 'metadata.csv')\n",
    "        self.metadata = pl.read_csv(self.metadata_path).rows(named=True)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # microtubule fluorescence,  Blue (B) channel\n",
    "        # endoplasmic reticulum,  Green (G) channel\n",
    "        # DNA, Red (R) channel\n",
    "        # Protein of interest, Alpha (A) channel\n",
    "        # https://virtualcellmodels.cziscience.com/dataset/01933229-3c87-7818-be80-d7e5578bb0b7\n",
    "        row = self.metadata[idx]\n",
    "        plate = str(row['if_plate_id'])\n",
    "        position = row['position']\n",
    "        sample = str(row['sample'])\n",
    "        cell_id = str(int(row['cell_id']))\n",
    "        image = os.path.join(self.root_dir, plate, f\"{plate}_{position}_{sample}_{cell_id}_cell_image.png\")\n",
    "        image = decode_image(image, mode='RGBA'), decode_image(mask)\n",
    "        \n",
    "        # bool_mask = (mask>0).expand((image.shape[0], -1, -1))\n",
    "        # return torch.where(bool_mask, image, 0)\n",
    "        return image\n",
    "        \n",
    "dataset = UnZippedImageArchive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fd6757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr/vidit/conda_env/lib/python3.10/site-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/scr/vidit/conda_env/lib/python3.10/site-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.v2.functional import center_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e433d2d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m UnZippedImageArchive()\n\u001b[0;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# 4, 1024, 1024\u001b[39;00m\n\u001b[1;32m      4\u001b[0m channels \u001b[38;5;241m=\u001b[39m [channel\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m channel \u001b[38;5;129;01min\u001b[39;00m image]\n\u001b[1;32m      5\u001b[0m concatenated_image \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(channels[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 35\u001b[0m, in \u001b[0;36mUnZippedImageArchive.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m image \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, plate, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcell_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cell_image.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m mask \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_dir, plate, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mposition\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcell_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_cell_mask.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 35\u001b[0m image, mask \u001b[38;5;241m=\u001b[39m \u001b[43mdecode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRGBA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, decode_image(mask)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# bool_mask = (mask>0).expand((image.shape[0], -1, -1))\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# return torch.where(bool_mask, image, 0)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image\n",
      "File \u001b[0;32m/scr/vidit/conda_env/lib/python3.10/site-packages/torchvision/io/image.py:236\u001b[0m, in \u001b[0;36mdecode_image\u001b[0;34m(input, mode)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[1;32m    235\u001b[0m     _log_api_usage_once(decode_image)\n\u001b[0;32m--> 236\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mdecode_image(\u001b[38;5;28minput\u001b[39m, \u001b[43mmode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'value'"
     ]
    }
   ],
   "source": [
    "dataset = UnZippedImageArchive()\n",
    "image = dataset[5] # 4, 1024, 1024\n",
    "\n",
    "channels = [channel.unsqueeze(0) for channel in image]\n",
    "concatenated_image = torch.cat(channels[:-1], dim=0)\n",
    "image_data_rgb = np.transpose(concatenated_image, (1, 2, 0))\n",
    "\n",
    "image_data_rgb.shape\n",
    "plt.imshow(image_data_rgb)\n",
    "plt.axis('off')\n",
    "plt.savefig(f'full_slide_hpa.png', bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "# for ch in range(concatenated_image.shape[0]):\n",
    "#     channel = concatenated_image[ch, : , :]\n",
    "#     plt.imshow(channel, cmap='grey')\n",
    "#     plt.axis('off')\n",
    "#     plt.savefig(f'ch_{ch}.png', bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff344950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "load_features = torch.load('/scr/data/HPA_features/all_features.pth')\n",
    "type(load_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4069c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1748545536])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_features[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a8f01b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
