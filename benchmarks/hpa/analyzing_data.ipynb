{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3755a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metadata = pd.read_csv(\"/scr/data/cell_crops/metadata.csv\")\n",
    "\n",
    "#studies = open(\"test_antibodies.txt\", 'r').readlines()\n",
    "#studies = [s.strip() for s in studies if s.strip()]\n",
    "#filtered_antibodies = metadata[metadata['antibody'].isin(studies)]\n",
    "#filtered_antibodies = filtered_antibodies.drop(columns=['Unnamed: 0'])\n",
    "#filtered_antibodies\n",
    "metadata = metadata.drop(columns=['Unnamed: 0'])\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab414e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out the encoding of images from the metadata\n",
    "import torch\n",
    "import numpy as np\n",
    "# metadata csv file -> {plate_number}_{position}_{sample}_cell_bbox.csv\n",
    "# png image -> {plate_number}_{position}_{sample}_{cell_id}_cell_bbox.png\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import os\n",
    "\n",
    "image = io.imread(\"/scr/data/cell_crops/1573/1573_B5_3_5_cell_image.png\")\n",
    "image_data = image[:, :, [0, 1, 3]]  \n",
    "\n",
    "plt.imshow(image_data)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "mask = io.imread(\"/scr/data/cell_crops/1573/1573_B5_3_5_cell_mask.png\")\n",
    "plt.imshow(mask)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "extra_image = cv2.imread(\"/scr/data/cell_crops/1/1_B2_1_5_cell_image.png\", -1)\n",
    "extra_image = np.transpose(extra_image, (2, 0, 1))  # Convert to CxHxW format\n",
    "extra_image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66571b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create annotations directory\n",
    "os.makedirs('annotations', exist_ok=True)\n",
    "\n",
    "# Standard HPA locations (matches what the script expects)\n",
    "hpa_locations = [\n",
    "    \"Actin filaments\", \"Aggresome\", \"Centrosome\", \"Cytosol\", \n",
    "    \"Endoplasmic reticulum\", \"Golgi apparatus\", \"Intermediate filaments\",\n",
    "    \"Microtubules\", \"Mitochondria\", \"Mitotic spindle\", \"Nuclear bodies\",\n",
    "    \"Nuclear membrane\", \"Nuclear speckles\", \"Nucleoli\", \n",
    "    \"Nucleoli fibrillar center\", \"Nucleoplasm\", \"Plasma membrane\", \n",
    "    \"Vesicles\", \"Cleavage furrow\", \"Midbody ring\", \"Rods & Rings\", \n",
    "    \"Microtubule ends\"\n",
    "]\n",
    "\n",
    "pd.DataFrame({'Original annotation': hpa_locations}).to_csv(\n",
    "    '/scr/vidit/FoundationModelBenchmarks/hpa/location_group_mapping.csv', index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a8b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "import os\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import decode_image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import sys\n",
    "from torchvision.transforms import v2\n",
    "from accelerate import Accelerator\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Custom collate function to handle None values\"\"\"\n",
    "    # Filter out None values\n",
    "    valid_batch = []\n",
    "    for item in batch:\n",
    "        if item[0] is not None and item[1] is not None:\n",
    "            valid_batch.append(item)\n",
    "        else:\n",
    "            print(f\"Skipping None item in batch\")\n",
    "    \n",
    "    if len(valid_batch) == 0:\n",
    "        print(\"WARNING: Empty batch after filtering None values\")\n",
    "        return None, None\n",
    "    \n",
    "    images, rows = zip(*valid_batch)\n",
    "    \n",
    "    try:\n",
    "        # Convert to tensors if needed and stack\n",
    "        tensor_images = []\n",
    "        for img in images:\n",
    "            if isinstance(img, np.ndarray):\n",
    "                tensor_images.append(torch.from_numpy(img).float())\n",
    "            elif isinstance(img, torch.Tensor):\n",
    "                tensor_images.append(img.float())\n",
    "            else:\n",
    "                print(f\"Unexpected image type: {type(img)}\")\n",
    "                continue\n",
    "        \n",
    "        if len(tensor_images) == 0:\n",
    "            return None, None\n",
    "            \n",
    "        images_tensor = torch.stack(tensor_images)\n",
    "        return images_tensor, list(rows)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in collate function: {e}\")\n",
    "        return None, None\n",
    "\n",
    "'''\n",
    "Custom Class to load HPA images for feature extraction.\n",
    "'''\n",
    "class UnZippedImageArchive(Dataset):\n",
    "    \"\"\"Basic unzipped image arch. This will no longer be used. \n",
    "       Remove when unzipped support is added to the IterableImageArchive\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir: str= '/scr/data/cell_crops/', transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.root_dir = root_dir\n",
    "        self.metadata_path = os.path.join(self.root_dir, 'metadata.csv')\n",
    "        self.metadata = pl.read_csv(self.metadata_path).rows(named=True)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            # microtubule fluorescence,  Blue (B) channel\n",
    "            # endoplasmic reticulum,  Green (G) channel\n",
    "            # DNA, Red (R) channel\n",
    "            # Protein of interest, Alpha (A) channel\n",
    "            # https://virtualcellmodels.cziscience.com/dataset/01933229-3c87-7818-be80-d7e5578bb0b7\n",
    "            row = self.metadata[idx]\n",
    "            plate = str(row['if_plate_id'])\n",
    "            position = row['position']\n",
    "            sample = str(row['sample'])\n",
    "            cell_id = str(int(row['cell_id']))\n",
    "            #{plate_number}_{position}_{sample}_{cell_id}_cell_bbox.png\n",
    "            image_path = os.path.join(self.root_dir, plate, f\"{plate}_{position}_{sample}_{cell_id}_cell_image.png\")\n",
    "            \n",
    "            # Check if file exists\n",
    "            if not os.path.exists(image_path):\n",
    "                print(f\"Image not found: {image_path}\")\n",
    "                return None, None\n",
    "            \n",
    "            # Try to load the image\n",
    "            image = cv2.imread(image_path, -1)\n",
    "            \n",
    "            # Check if image loaded successfully\n",
    "            if image is None:\n",
    "                print(f\"Failed to load image (cv2.imread returned None): {image_path}\")\n",
    "                return None, None\n",
    "            \n",
    "            # Check if image has the expected shape\n",
    "            if len(image.shape) != 3 or image.shape[2] != 4:\n",
    "                print(f\"Unexpected image shape {image.shape} for {image_path}\")\n",
    "                return None, None\n",
    "            \n",
    "            # Transpose to (C, H, W) format\n",
    "            image = np.transpose(image, (2, 0, 1))\n",
    "            \n",
    "            # Apply transforms if provided\n",
    "            if self.transform:\n",
    "                # Convert to tensor for transforms\n",
    "                image_tensor = torch.from_numpy(image).float()\n",
    "                image = self.transform(image_tensor)\n",
    "            \n",
    "            return image, row\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading sample {idx}: {e}\")\n",
    "            return None, None\n",
    "\n",
    "# Test the dataset first\n",
    "print(\"Testing dataset...\")\n",
    "image_folder = \"/scr/data/cell_crops\"\n",
    "dataset = UnZippedImageArchive(root_dir=image_folder, transform=v2.Resize(size=(224, 224)))\n",
    "\n",
    "# Test a few samples\n",
    "print(\"Testing first 5 samples:\")\n",
    "valid_samples = 0\n",
    "for i in range(min(5, len(dataset))):\n",
    "    try:\n",
    "        img, row = dataset[i]\n",
    "        if img is not None and row is not None:\n",
    "            print(f\"Sample {i}: SUCCESS - Image shape: {img.shape}, Image type: {type(img)}\")\n",
    "            valid_samples += 1\n",
    "        else:\n",
    "            print(f\"Sample {i}: FAILED - Image or row is None\")\n",
    "    except Exception as e:\n",
    "        print(f\"Sample {i}: ERROR - {e}\")\n",
    "\n",
    "print(f\"Valid samples: {valid_samples}/5\")\n",
    "\n",
    "if valid_samples > 0:\n",
    "    print(\"Creating dataloader with custom collate function...\")\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, \n",
    "        batch_size=16, \n",
    "        shuffle=False, \n",
    "        num_workers=8,  # Set to 0 for debugging\n",
    "        collate_fn=custom_collate_fn\n",
    "    )\n",
    "    \n",
    "    print(\"Testing dataloader...\")\n",
    "    with torch.no_grad():\n",
    "        for i, (batch, rows) in enumerate(tqdm(dataloader, desc=\"Extracting features\")):\n",
    "            if batch is not None and rows is not None:\n",
    "                print(f\"Batch {i}: SUCCESS - Shape: {batch.shape}, Rows: {len(rows)}\")\n",
    "            else:\n",
    "                print(f\"Batch {i}: FAILED - Batch or rows is None\")\n",
    " \n",
    "    \n",
    "    print(\"Dataloader test complete!\")\n",
    "else:\n",
    "    print(\"No valid samples found. Please check your data paths and files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ae3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "hpa_df = pd.read_csv(\"/scr/data/cell_crops/metadata.csv\")\n",
    "image_folder = \"/scr/data/cell_crops\"\n",
    "\n",
    "for idx, row in hpa_df.iterrows():\n",
    "    image_path = os.path.join(\n",
    "        image_folder, \n",
    "        str(row[\"if_plate_id\"]), \n",
    "        f\"{row['if_plate_id']}_{row['position']}_{row['sample']}_{int(row['cell_id'])}_cell_image.png\"\n",
    "    )\n",
    "    image = cv2.imread(image_path, -1)\n",
    "    if image is None:\n",
    "        print(f\"Image not found or could not be read: {image_path}\")\n",
    "        continue\n",
    "    \n",
    "    # You can add more processing here if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcf61a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
